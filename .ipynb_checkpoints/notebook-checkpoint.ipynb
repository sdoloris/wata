{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believe movie piece art one best movie seen while\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files/Java/jre1.8.0_191/bin/java.exe\"   \n",
    "os.environ['JAVAHOME'] = java_path\n",
    "st = StanfordPOSTagger('./stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger', path_to_jar='./stanford-postagger-2018-10-16/stanford-postagger.jar', java_options='-Xmx2048m')\n",
    "\n",
    "def preprocess(text, stoplist):\n",
    "    # To lower-case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = ' '.join([i for i in text.split() if i not in stoplist])\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    \n",
    "    # Lemmatization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = [wordnet_lemmatizer.lemmatize(i) for i in text.split()]\n",
    "    \n",
    "    # Pos tagging\n",
    "    '''tag = st.tag(text)\n",
    "    text = ['{}_{}'.format(i[0], i[1]) for i in tag]'''\n",
    "    \n",
    "    return ' '.join(text)\n",
    "\n",
    "stoplist = open(\"stoplist.txt\", \"r\").read().split()\n",
    "print(preprocess(\"I believe that this movie was a piece of art. This was one of the best movies I have seen in a while.\", stoplist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    doc label\n",
      "0     film adapted comic book plenty success whether...     1\n",
      "1     every now movie come along suspect studio ever...     1\n",
      "2     got mail work alot better deserves order make ...     1\n",
      "3     jaw rare film grab attention show single image...     1\n",
      "4     moviemaking lot like general manager nfl team ...     1\n",
      "5     june 30 1960 selftaught idealistic yet pragmat...     1\n",
      "6     apparently director tony kaye major battle new...     1\n",
      "7     one colleague surprised told willing see betsy...     1\n",
      "8     bloody clash independence won lumumba refused ...     1\n",
      "9     american action film slowly drowning death sea...     1\n",
      "10    watching rat race last week noticed cheek sore...     1\n",
      "11    noticed something lately never thought pseudo ...     1\n",
      "12    synopsis bobby garfield yelchin life small tow...     1\n",
      "13    synopsis movie steven spielberg one today fine...     1\n",
      "14    police negotiator person entirely unenviable j...     1\n",
      "15    plot young man love heavy metal music especial...     1\n",
      "16    carry matron last great carryon film opinion m...     1\n",
      "17    ultimate match good evil untouchable excellent...     1\n",
      "18    seen framed roger rabbit 10 year remembering m...     1\n",
      "19    something ben stiller make popular choice amon...     1\n",
      "20    phil curtolo mel gibson braveheart gave grippi...     1\n",
      "21    one can observe star trek movie expect see ser...     1\n",
      "22    fully loaded entertainment review website comi...     1\n",
      "23    bulworth ended allowed sigh relief possible en...     1\n",
      "24    call 911 cliche police must eye window soul fi...     1\n",
      "25    hilarious ultralow budget comedy film school d...     1\n",
      "26    u yet born 1960s rock n rolled around monterey...     1\n",
      "27    common many case complaint francis ford coppol...     1\n",
      "28    blair witch project perhaps one kind unique fi...     1\n",
      "29    seen may 31 1999 home video rented one best th...     1\n",
      "...                                                 ...   ...\n",
      "1970  disney sticktowhatyoudobest rule state disney ...     0\n",
      "1971  twin surfer dude stew phil deedle lay bandaged...     0\n",
      "1972  bicentennial man family film without external ...     0\n",
      "1973  continuation warner brother franchise joel sch...     0\n",
      "1974  long ago film constructed strong dialogue orig...     0\n",
      "1975  star armand assante mike hammer barbara carrer...     0\n",
      "1976  nostalgia 70 continues see revival one decade ...     0\n",
      "1977  saw film christmas day expecting upbeat comedy...     0\n",
      "1978  anna king least fourth film adaptation margare...     0\n",
      "1979  robin hood men tights another melbrooksproduce...     0\n",
      "1980  successful book movie michael crichton well ea...     0\n",
      "1981  director luis mandokis last film superb seriou...     0\n",
      "1982  remember really enjoying movie saw year ago gu...     0\n",
      "1983  tommy lee jones chase innocent victim around a...     0\n",
      "1984  fond writer use cheap easy pun completely usin...     0\n",
      "1985  boy great movie keanu reef morgan freeman acti...     0\n",
      "1986  u think leslie nielsen bumbling hapless straig...     0\n",
      "1987  please mind windbag letting bit steam just wan...     0\n",
      "1988  movie written man deemed one hottest writer ho...     0\n",
      "1989  typical cinematic high school football jock se...     0\n",
      "1990  man one wierd movie similar conspiracy theory ...     0\n",
      "1991  king warner brother animated musical feature r...     0\n",
      "1992  synopsis cromagnon ayla loses mother earthquak...     0\n",
      "1993  salary hollywood top actor getting obscenely l...     0\n",
      "1994  movie like six day seven night make mad talent...     0\n",
      "1995  anything stigma taken warning releasing simila...     0\n",
      "1996  john boormans zardoz goofy cinematic debacle f...     0\n",
      "1997  kid hall acquired taste took least season watc...     0\n",
      "1998  time john carpenter great horror director cour...     0\n",
      "1999  two party guy bob head haddaways dance hit lov...     0\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "collection = pd.DataFrame(columns=['doc', 'label'])\n",
    "\n",
    "stoplist = open(\"stoplist.txt\", \"r\").read().split()\n",
    "\n",
    "for file_name in os.listdir('./txt_sentoken/pos'):\n",
    "    file = open('./txt_sentoken/pos/' + file_name)\n",
    "    collection = collection.append(pd.DataFrame({'doc':[preprocess(file.read(), stoplist)], 'label':[1]}), ignore_index=True)\n",
    "\n",
    "    \n",
    "for file_name in os.listdir('./txt_sentoken/neg'):\n",
    "    file = open('./txt_sentoken/neg/' + file_name)\n",
    "    collection = collection.append(pd.DataFrame({'doc':[preprocess(file.read(), stoplist)], 'label':[0]}), ignore_index=True)\n",
    "\n",
    "print(collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06380178 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "film            0.054671\n",
      "movie           0.040041\n",
      "one             0.029536\n",
      "character       0.023156\n",
      "like            0.021646\n",
      "just            0.019083\n",
      "get             0.018427\n",
      "time            0.018057\n",
      "scene           0.017939\n",
      "story           0.017027\n",
      "good            0.016907\n",
      "even            0.016769\n",
      "make            0.016727\n",
      "will            0.016268\n",
      "can             0.015404\n",
      "much            0.014676\n",
      "life            0.014614\n",
      "also            0.014371\n",
      "see             0.014221\n",
      "two             0.013814\n",
      "well            0.013564\n",
      "thing           0.013420\n",
      "really          0.013385\n",
      "way             0.013355\n",
      "go              0.013351\n",
      "first           0.013234\n",
      "plot            0.013177\n",
      "bad             0.012845\n",
      "year            0.012495\n",
      "know            0.012421\n",
      "                  ...   \n",
      "penis           0.000556\n",
      "pete            0.000556\n",
      "bruno           0.000556\n",
      "detroit         0.000556\n",
      "mixture         0.000556\n",
      "moon            0.000556\n",
      "broadway        0.000556\n",
      "1998s           0.000555\n",
      "suvari          0.000555\n",
      "favourite       0.000555\n",
      "fishing         0.000555\n",
      "displayed       0.000555\n",
      "placement       0.000555\n",
      "resource        0.000555\n",
      "fiancee         0.000555\n",
      "effortlessly    0.000555\n",
      "leslie          0.000555\n",
      "temple          0.000555\n",
      "invited         0.000555\n",
      "lester          0.000555\n",
      "incidentally    0.000555\n",
      "constructed     0.000555\n",
      "leder           0.000555\n",
      "muscle          0.000555\n",
      "trained         0.000555\n",
      "imitation       0.000555\n",
      "survives        0.000554\n",
      "buff            0.000554\n",
      "sergeant        0.000554\n",
      "eats            0.000554\n",
      "Length: 4700, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Using sklearn's TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "result = vectorizer.fit_transform(collection['doc'])\n",
    "\n",
    "print(result.toarray())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tfidf = pd.DataFrame(result.toarray(), index=range(result.shape[0]), columns=vectorizer.get_feature_names())\n",
    "mean_vector = tfidf.mean().sort_values(ascending=False)\n",
    "mean = mean_vector.mean()\n",
    "selected_threshold = 0.0005540915680876647\n",
    "\n",
    "\n",
    "selected_features = mean_vector[mean_vector > selected_threshold]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "p = plt.bar(range(len(mean_vector)), mean_vector.values, log=True)\n",
    "plt.axhline(y=mean_vector.mean(), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[880 120]\n",
      " [295 705]]\n",
      "0.7925\n",
      "[[852 148]\n",
      " [179 821]]\n",
      "0.8365\n"
     ]
    }
   ],
   "source": [
    "# to \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Building the Vector State Model using frequency vectorizer\n",
    "vectorizer = CountVectorizer(vocabulary = selected_features.index)\n",
    "X = vectorizer.fit_transform(collection['doc'])\n",
    "X = X.toarray()\n",
    "y = collection['label'].astype(int)  # cast needed as pandas consider cells as 'object'\n",
    "\n",
    "# Further improve the tests using Xhi²\n",
    "selectkbest = SelectKBest(chi2, k=1000)  # We keep the 1000 most promising features\n",
    "X = selectkbest.fit_transform(X, y)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Easier solution below\n",
    "\n",
    "gnb_conf_mat = np.zeros((2,2))\n",
    "rf_conf_mat = np.zeros((2,2))\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    gnb.fit(X_train, y_train)\n",
    "    gnb_conf_mat += confusion_matrix(y_test, gnb.predict(X_test))\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_conf_mat += confusion_matrix(y_test, rf.predict(X_test))\n",
    "'''\n",
    "\n",
    "y_predict_gnb = cross_val_predict(gnb, X, y, cv=kf, n_jobs=-1)\n",
    "y_predict_rf = cross_val_predict(rf, X, y, cv=kf, n_jobs=-1)\n",
    "\n",
    "gnb_conf_mat = confusion_matrix(y, y_predict_gnb)\n",
    "rf_conf_mat = confusion_matrix(y, y_predict_rf)\n",
    "\n",
    "print(gnb_conf_mat)\n",
    "print(accuracy_score(y, y_predict_gnb))\n",
    "\n",
    "print(rf_conf_mat)\n",
    "print(accuracy_score(y, y_predict_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate on new reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  doc label\n",
      "0   “but time different” say two different charact...     1\n",
      "1   lushly shot gorgeous location film evokes tone...     1\n",
      "2   updating u one rockys fearsome opponent sequel...     1\n",
      "3   guy step ring huge underdog sure he’s got grit...     1\n",
      "4   sylvester stallone ever tire wonder playing fa...     1\n",
      "5   three year ago delighted see creed continuatio...     1\n",
      "6   2015 ryan coogler’s creed came nowhere become ...     1\n",
      "7   “it’s almost shakespearean” announcer say can’...     1\n",
      "8   1993 twelveyearold giuseppe di matteo kidnappe...     1\n",
      "9   divine blend natural supernatural fantasy actu...     1\n",
      "10  possession hannah grace begin comprehensive as...     0\n",
      "11  arriving just time win place among year’s wors...     0\n",
      "12  don’t horror aficionado know corps scary lifel...     0\n",
      "13  theory putting “possession” title horror movie...     0\n",
      "14  job working graveyard shift morgue go wrong sh...     0\n",
      "15  ever watch movie wonder got elusive green ligh...     0\n",
      "16  possession hannah grace lot waiting something ...     0\n",
      "17  legendary outlaw returned time spiderman versi...     0\n",
      "18  outset “robin hood” 2018 edition narrator tell...     0\n",
      "19  build suspension bridge gap robin hood is hero...     0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "new_reviews = pd.DataFrame(columns=['doc', 'label'])\n",
    "\n",
    "stoplist = open(\"stoplist.txt\", \"r\").read().split()\n",
    "\n",
    "for file_name in os.listdir('./new_reviews/pos'):\n",
    "    file = open('./new_reviews/pos/' + file_name, encoding='utf8')\n",
    "    new_reviews = new_reviews.append(pd.DataFrame({'doc':[preprocess(file.read(), stoplist)], 'label':[1]}), ignore_index=True)\n",
    "\n",
    "    \n",
    "for file_name in os.listdir('./new_reviews/neg'):\n",
    "    file = open('./new_reviews/neg/' + file_name, encoding='utf8')\n",
    "    new_reviews = new_reviews.append(pd.DataFrame({'doc':[preprocess(file.read(), stoplist)], 'label':[0]}), ignore_index=True)\n",
    "\n",
    "print(new_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "[[9 1]\n",
      " [2 8]]\n",
      "0.9\n",
      "[[10  0]\n",
      " [ 2  8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# First we have to retrain the models on the whole dataset\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rf.fit(X, y.values)\n",
    "gnb.fit(X, y.values)\n",
    "\n",
    "X_news = vectorizer.transform(new_reviews['doc'])\n",
    "X_news = X_news.toarray()\n",
    "X_news = selectkbest.transform(X_news)\n",
    "y_news = new_reviews['label'].astype(int)\n",
    "\n",
    "news_predict_gnb = gnb.predict(X_news)\n",
    "news_predict_rf = rf.predict(X_news)\n",
    "\n",
    "# Confusion matrices\n",
    "print(accuracy_score(y_news, news_predict_gnb))\n",
    "print(confusion_matrix(y_news, news_predict_gnb))\n",
    "print(accuracy_score(y_news, news_predict_rf))\n",
    "print(confusion_matrix(y_news, news_predict_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the optimal threshold (of tfidf values) by looking at the TPR - FPR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tpr_fpr(estimator, collection, tfidf_mean, threshold):\n",
    "    selected_features = tfidf_mean[tfidf_mean > threshold]\n",
    "    vectorizer = CountVectorizer(vocabulary = selected_features.index)\n",
    "    X = vectorizer.fit_transform(collection['doc'])\n",
    "    X = X.toarray()\n",
    "    y = collection['label'].astype(int)  # cast needed as pandas consider cells as 'object'\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test, y_predict)\n",
    "    \n",
    "    tn, fp, fn, tp = conf_mat[0,0], conf_mat[0,1], conf_mat[1,0], conf_mat[1,1]\n",
    "\n",
    "    return tp/(tp+fn), fp/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.456892826398864e-06\n",
      "0.0005540915680876647\n",
      "0.0011007262433489306\n",
      "0.0016473609186101964\n",
      "0.0021939955938714627\n",
      "0.0027406302691327287\n",
      "0.0032872649443939942\n",
      "0.0038338996196552603\n",
      "0.004380534294916526\n",
      "0.004927168970177792\n",
      "0.005473803645439058\n",
      "0.006020438320700324\n",
      "0.006567072995961589\n",
      "0.0071137076712228554\n",
      "0.0076603423464841214\n",
      "0.008206977021745387\n",
      "0.008753611697006653\n",
      "0.00930024637226792\n",
      "0.009846881047529185\n",
      "0.010393515722790451\n",
      "0.010940150398051718\n",
      "0.011486785073312984\n",
      "0.01203341974857425\n",
      "0.012580054423835516\n",
      "0.01312668909909678\n",
      "0.013673323774358046\n",
      "0.014219958449619312\n",
      "0.014766593124880578\n",
      "0.015313227800141844\n",
      "0.01585986247540311\n",
      "0.016406497150664376\n",
      "0.016953131825925644\n",
      "0.017499766501186908\n",
      "0.018046401176448172\n",
      "0.01859303585170944\n",
      "0.019139670526970704\n",
      "0.019686305202231972\n",
      "0.020232939877493236\n",
      "0.020779574552754504\n",
      "0.021326209228015768\n",
      "0.021872843903277036\n",
      "0.0224194785785383\n",
      "0.022966113253799568\n",
      "0.023512747929060832\n",
      "0.0240593826043221\n",
      "0.024606017279583364\n",
      "0.025152651954844632\n",
      "0.025699286630105896\n",
      "0.02624592130536716\n",
      "0.02679255598062843\n",
      "0.027339190655889693\n",
      "0.02788582533115096\n",
      "0.028432460006412225\n",
      "0.028979094681673492\n",
      "0.029525729356934757\n",
      "0.030072364032196024\n",
      "0.03061899870745729\n",
      "0.031165633382718556\n",
      "0.031712268057979824\n",
      "0.032258902733241085\n",
      "0.03280553740850235\n",
      "0.03335217208376362\n",
      "0.03389880675902489\n",
      "0.03444544143428615\n",
      "0.03499207610954742\n",
      "0.035538710784808684\n",
      "0.036085345460069945\n",
      "0.03663198013533121\n",
      "0.03717861481059248\n",
      "0.03772524948585375\n",
      "0.03827188416111501\n",
      "0.03881851883637628\n",
      "0.039365153511637545\n",
      "0.03991178818689881\n",
      "0.04045842286216007\n",
      "0.04100505753742134\n",
      "0.04155169221268261\n",
      "0.04209832688794388\n",
      "0.04264496156320514\n",
      "0.043191596238466405\n",
      "0.04373823091372767\n",
      "0.044284865588988934\n",
      "0.0448315002642502\n",
      "0.04537813493951147\n",
      "0.04592476961477274\n",
      "0.046471404290034\n",
      "0.047018038965295265\n",
      "0.04756467364055653\n",
      "0.0481113083158178\n",
      "0.04865794299107906\n",
      "0.04920457766634033\n",
      "0.0497512123416016\n",
      "0.050297847016862865\n",
      "0.050844481692124126\n",
      "0.051391116367385394\n",
      "0.05193775104264666\n",
      "0.05248438571790792\n",
      "0.05303102039316919\n",
      "0.05357765506843046\n",
      "0.054124289743691725\n",
      "[0.81, 0.84, 0.83, 0.81, 0.81, 0.82, 0.79, 0.76, 0.75, 0.71, 0.73, 0.71, 0.71, 0.725, 0.725, 0.705, 0.705, 0.73, 0.71, 0.725, 0.675, 0.65, 0.695, 0.68, 0.615, 0.605, 0.605, 0.62, 0.58, 0.575, 0.57, 0.58, 0.56, 0.565, 0.565, 0.565, 0.55, 0.555, 0.545, 0.545, 0.55, 0.525, 0.52, 0.53, 0.565, 0.5, 0.535, 0.54, 0.525, 0.53, 0.525, 0.525, 0.53, 0.56, 0.505, 0.535, 0.565, 0.545, 0.565, 0.555, 0.57, 0.565, 0.55, 0.54, 0.53, 0.53, 0.55, 0.6, 0.505, 0.53, 0.55, 0.545, 0.505, 0.565, 0.615, 0.52, 0.545, 0.455, 0.455, 0.52, 0.52, 0.52, 0.455, 0.45, 0.455, 0.52, 0.455, 0.455, 0.52, 0.455, 0.52, 0.45, 0.485, 0.55, 0.455, 0.455, 0.52, 0.52, 0.52, 0.52]\n",
      "[0.17, 0.185, 0.18, 0.19, 0.21, 0.265, 0.305, 0.23, 0.29, 0.26, 0.23, 0.28, 0.255, 0.31, 0.26, 0.325, 0.295, 0.335, 0.335, 0.345, 0.315, 0.36, 0.38, 0.37, 0.36, 0.37, 0.355, 0.495, 0.46, 0.42, 0.425, 0.465, 0.48, 0.44, 0.465, 0.425, 0.435, 0.425, 0.39, 0.425, 0.48, 0.505, 0.49, 0.52, 0.51, 0.53, 0.525, 0.53, 0.53, 0.53, 0.485, 0.535, 0.535, 0.535, 0.495, 0.46, 0.425, 0.44, 0.44, 0.485, 0.44, 0.44, 0.44, 0.455, 0.435, 0.44, 0.455, 0.5, 0.44, 0.46, 0.455, 0.445, 0.415, 0.465, 0.585, 0.485, 0.53, 0.43, 0.43, 0.485, 0.485, 0.485, 0.43, 0.43, 0.43, 0.485, 0.43, 0.43, 0.485, 0.43, 0.485, 0.43, 0.44, 0.53, 0.43, 0.43, 0.485, 0.485, 0.485, 0.485]\n"
     ]
    }
   ],
   "source": [
    "min_threshold = mean_vector.min()\n",
    "max_threshold = mean_vector.max()\n",
    "\n",
    "thresholds = np.arange(min_threshold, max_threshold, (max_threshold-min_threshold)/100)\n",
    "TPRs, FPRs = [], []\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "for threshold in thresholds:\n",
    "    tpr, fpr = tpr_fpr(rf, collection, mean_vector, threshold)\n",
    "    TPRs.append(tpr)\n",
    "    FPRs.append(fpr)\n",
    "print(TPRs)\n",
    "print(FPRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "TPRs_np = np.array(TPRs)\n",
    "FPRs_np = np.array(FPRs)\n",
    "\n",
    "\n",
    "selected_threshold_idx = np.argmin(np.sqrt((TPRs_np-1)**2 + (FPRs_np-0)**2))\n",
    "selected_threshold = thresholds[selected_threshold_idx]\n",
    "\n",
    "plt.title('ROC plot for varying threshold')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.scatter(FPRs, TPRs)\n",
    "plt.scatter(FPRs[selected_threshold_idx], TPRs[selected_threshold_idx], s=20, c='r')\n",
    "plt.plot([0,1], [0,1], 'black')\n",
    "\n",
    "print(\"Selected threshold =\", selected_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def tf(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def idf(term, corpus):\n",
    "    return np.log(len(corpus['doc'])/len(corpus[corpus['doc'].str.contains(term)]))\n",
    "\n",
    "tf('tree', preprocess(\"this is a tree, and a big tree, not a three\", stoplist))\n",
    "\n",
    "def tf_idf(corpus):\n",
    "    print(\"start\")\n",
    "    words = set()\n",
    "    for doc in corpus['doc']:\n",
    "        for word in doc.split():\n",
    "            words.add(word)\n",
    "    \n",
    "    tf_idfs = pd.DataFrame(index=range(corpus.shape[0]-1), columns=words)\n",
    "    \n",
    "    idfs = {}\n",
    "    for word in words:\n",
    "        idfs[word] = idf(word, corpus)\n",
    "    \n",
    "    print('started tf_idf')\n",
    "    \n",
    "    for doc_idx in range(corpus.shape[0]):\n",
    "        doc = corpus.iloc[doc_idx]['doc']\n",
    "        for word in words:\n",
    "            tf_idfs.at[doc_idx, word] = tf(word, doc) * idfs[word]\n",
    "    print('finished')\n",
    "            \n",
    "    return tf_idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
